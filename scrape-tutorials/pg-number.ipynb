{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d6aee8",
   "metadata": {},
   "source": [
    "# Recursively Finding the PG Number\n",
    "\n",
    "<br/>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.webtranspose.com/\">\n",
    "  <img src=\"https://www.pgnumber.com/_next/image?url=%2Fbuilt-with-black-new.png&w=384&q=75\" alt=\"Open In Colab\" style='max-width:20%'/>\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "Using [Web Transpose AI Web Scraper](https://webtranspose.com/ai-web-scraper) to extract the people thanked by Paul Graham in his blog posts.\n",
    "\n",
    "Then using [Web Transpose AI SERP API](https://www.webtranspose.com/) [Web Transpose Web Crawl](https://www.webtranspose.com/distributed-cloud-web-crawler) to recursively extract these people's blogs to create a graph of closeness to Paul Graham.\n",
    "\n",
    "- [Visualization (PG Number: pgnumber.com)](https://pgnumber.com/)\n",
    "- [Blog Post](https://www.webtranspose.com/blog/examples/pg-number)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mike-gee/webtranspose-tutorials-python/blob/main/scrape-tutorials/pg-number.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d40104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install webtranspose --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638acefd",
   "metadata": {},
   "source": [
    "## 0. Setup & Get API Key\n",
    "\n",
    "Import Web Transpose. You can get a **free Web Transpose API Key** [here](https://app.webtranspose.com/dashboard).\n",
    "\n",
    "https://app.webtranspose.com/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import webtranspose as webt\n",
    "\n",
    "os.environ[\"WEBTRANSPOSE_API_KEY\"] = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb3ef5",
   "metadata": {},
   "source": [
    "## 1. Crawling the Web Page\n",
    "\n",
    "This will download Paul Graham's Website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad61d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = webt.Crawl(\n",
    "    'http://paulgraham.com/',\n",
    "    max_pages=10, # set to 10 for testing - set this to 500 when you want to run this for real.\n",
    "    render_js=False,\n",
    ")\n",
    "crawl.queue_crawl()\n",
    "print('Crawl ID:', crawl.crawl_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Recover the Crawl ID if required\n",
    "crawl = webt.get_crawl(\"YOUR CRAWL ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630b4d2",
   "metadata": {},
   "source": [
    "## 2. Scraping the Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a4f6d",
   "metadata": {},
   "source": [
    "First we define a schema of what we wish to extract from the website. We then build an AI web scraper. This will work on any website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'page classification': ['blog / essay', 'other type of page'],\n",
    "    'people thanked': {\n",
    "        'type': 'array',\n",
    "        'items': {\n",
    "            'person name': 'string',\n",
    "            'reason mentioned': ['thanked for reading drafts of this blog / essay', 'other kind of praise', 'other reason']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "scraper = webt.Scraper(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd846ec",
   "metadata": {},
   "source": [
    "Now, we loop through all the pages from the web crawl and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275eab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = []\n",
    "for i, url in enumerate(crawl.get_visited()):\n",
    "    if i % 2 == 0:\n",
    "        print(i)\n",
    "    page = crawl.get_page(url)\n",
    "    # We pass in the HTML we already extracted in the call\n",
    "    # You can also just pass in the URL and Web Transpose will scrape the web page\n",
    "    out_data = scraper.scrape(url, html=page['html']) \n",
    "    if out_data['page classification'] == 'blog / essay':\n",
    "        essays.append({\n",
    "            'url': url,\n",
    "            'people thanked': out_data['people thanked']\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b19f2",
   "metadata": {},
   "source": [
    "## 3. Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c045541",
   "metadata": {},
   "source": [
    "Get all unique people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = set()\n",
    "for essay in essays:\n",
    "    for person in essay['people thanked']:\n",
    "        if person['reason mentioned'] == 'thanked for reading drafts of this blog / essay':\n",
    "            people.add(person['person name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a1036",
   "metadata": {},
   "source": [
    "Get everyone's blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c349ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_dict = {}\n",
    "for person_name in list(people):\n",
    "    results = webt.search_filter(f\"{person_name}'s blog\")\n",
    "    if len(results['filtered_results']) > 0:\n",
    "        blog_dict[person_name] = [x['url'] for x in results['filtered_results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e56eca",
   "metadata": {},
   "source": [
    "## 4. Recurse\n",
    "\n",
    "Now re-run the above code on all these new websites. \n",
    "\n",
    "It's going to be most efficient to crawl all the websites in parallel and then scrape them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce9d1d",
   "metadata": {},
   "source": [
    "### Crawl all the new websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e560b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_crawl_dict = {}\n",
    "\n",
    "for name, websites in blog_dict.items():\n",
    "    for url in websites:\n",
    "        crawl = webt.Crawl(url, max_pages=100)\n",
    "        crawl.queue_crawl()\n",
    "        website_crawl_dict[url] = crawl.crawl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl.max_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl.get_queued()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62474a",
   "metadata": {},
   "source": [
    "### Wait for Crawling to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "complete = False\n",
    "\n",
    "while not complete:\n",
    "    for url, crawl_id in website_crawl_dict.items():\n",
    "        crawl = webt.get_crawl(crawl_id)\n",
    "        if len(crawl.get_visited()) < crawl.max_pages and len(crawl.get_queued()) > 0:\n",
    "            print(f\"Crawl for {url} is still running...\")\n",
    "    sleep(30)\n",
    "    print(\"Waiting for crawls to finish...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17da7a",
   "metadata": {},
   "source": [
    "### Scrape Crawls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_essay_dict = {}\n",
    "\n",
    "for crawl_id in website_crawl_dict.values():\n",
    "    crawl = webt.get_crawl(crawl_id)\n",
    "    \n",
    "    crawl_essays = []\n",
    "    for i, url in enumerate(crawl.get_visited()):\n",
    "        if i % 2 == 0:\n",
    "            print(i)\n",
    "        page = crawl.get_page(url)\n",
    "        # We pass in the HTML we already extracted in the call\n",
    "        # You can also just pass in the URL and Web Transpose will scrape the web page\n",
    "        out_data = scraper.scrape(url, html=page['html']) \n",
    "        if out_data['page classification'] == 'blog / essay':\n",
    "            crawl_essays.append({\n",
    "                'url': url,\n",
    "                'people thanked': out_data['people thanked']\n",
    "            })\n",
    "            \n",
    "    crawl_essay_dict[crawl_id] = crawl_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2ca58",
   "metadata": {},
   "source": [
    "### Re-Gather People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for crawl_id in website_crawl_dict.values():\n",
    "    new_essays = crawl_essay_dict[crawl_id]\n",
    "    for essay in new_essays:\n",
    "        for person in essay['people thanked']:\n",
    "            if person['reason mentioned'] == 'thanked for reading drafts of this blog / essay':\n",
    "                people.add(person['person name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8197ab",
   "metadata": {},
   "source": [
    "## 5. Result\n",
    "\n",
    "You can view the results of running this recursively at:\n",
    "\n",
    "- [PG Number & Essay Graph](https://pgnumber.com/) (Visual Interface)\n",
    "- [Github Repo](https://github.com/mike-gee/pg-number) (Raw Data on Github)\n",
    "\n",
    "### Tools Used:\n",
    "\n",
    "- [Web Transpose](https://webtranspose.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7b4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
